{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZhHoVO3BpRm"
   },
   "source": [
    "# [이론6] Data Augmentation and Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6M8YNJnfCK_q"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cCuQhe78CKD2"
   },
   "source": [
    "## 학습 목표\n",
    "- 데이터 증강이 필요한 이유와 여러가지 데이터 증강 방법에 대해서 살펴봅니다.\n",
    "- 전이학습을 활용하는 방식에 대해 알아봅니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kMIrfqN-CVYi"
   },
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXA90AsOCOXs"
   },
   "source": [
    "## 목차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRl2pWf3COj8"
   },
   "source": [
    "#### 1. 데이터 증강 기법 (Data Augmentation)\n",
    "#### 2. 전이학습 (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBr9ou7PCWcN"
   },
   "source": [
    "-------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndQcKGLwqSMp"
   },
   "source": [
    "## 1. 데이터 증강 기법 (Data Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wYxDm4D7T2e_"
   },
   "source": [
    "AR(Augmented Reality)라는 말을 모두 한 번쯤 들어보셨나요? AR은 **증강 현실**이라는 뜻으로 실제로 존재하는 환경에 가상의 사물이나 정보를 합성하여 마치 원래의 환경에 존재하는 사물처럼 보이도록 하는 기법이죠. 이 기술을 활용한 '포켓몬 고' 라는 모바일 게임이 한창 유행하기도 했었습니다. 2021년에 애플에서 나름 강조해서 홍보했던 기술이기도 하며, 자동차 내비게이션 영역에도 점차 융합되고 있는 기술입니다.\n",
    "\n",
    "<center><img src=\"https://i0.wp.com/sharehows.com/wp-content/uploads/2020/08/spqlrpdltus.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hETw0OlxVQQ6"
   },
   "source": [
    "그렇다면 딥러닝에서 얘기하는 **데이터 증강(Data Augmentation)** 이라는 것은 무슨 뜻일까요? 이를 알기 위해서는 딥러닝 모델이 가지는 일반적인 특징을 짚고 넘어갈 필요가 있습니다. 현재 유명한 대부분의 딥러닝 모델은 그 성능을 증명하기 위하여 **압도적인 양의 데이터**를 사용합니다. 당장 이미지 분류 분야에서 유명한 ImageNet만 해도 100만장이 넘는 이미지를 통해 모델을 학습하고 성능을 평가합니다. 그러나 이는 반대로 **적은 수의 데이터로는 딥러닝 모델을 충분히 학습시키기 힘들다**는 의미가 되기도 합니다.\n",
    "\n",
    "<center><img src=\"http://t1.kakaocdn.net/braincloud/homepage/article_image/aae39e38-4145-43b8-abc5-1ec9ea529f1c.png\" align=\"center\" border=\"0\"  width=\"400\" height=auto></center>\n",
    "\n",
    "하지만 ImageNet 같은 벤치마크 데이터셋이 아닌 현실에 유용한 모델을 위한 데이터는 그와 같이 수많은 데이터를 모으는 것이 쉽지 않은 경우가 많습니다. 적은 수의 데이터로 딥러닝 모델을 학습시키고자 하면 데이터에 비해 모델의 표현력이 너무 좋기 때문에 **과적합(overfitting)** 문제에 취약해지기도 합니다. 이때 적은 수의 데이터를 강제로 늘려주는 기법을 바로 **데이터 증강(Data Augmentation)** 이라 부릅니다.\n",
    "\n",
    "물론 강제로 늘려준다고 하여 아무런 규칙 없이 마구잡이로 늘릴 수는 없을 것입니다. 양만 늘리고자 한다면 아예 상관이 없어보이는 두 데이터셋을 섞는 것도 한 방법이 되겠지만, 이래서는 딥러닝 모델을 학습하는 의미가 전혀 없을 것입니다. 그래서 데이터의 형태에 따라 데이터 증강을 수행하는 방법도 가지각색입니다. 여기서는 이미지 처리 알고리즘에 대해 알아보고 있으니 **이미지 데이터를 어떻게 증강하는지** 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 기본적인 이미지 데이터 증강"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnP5qVtQXfHQ"
   },
   "source": [
    "이미지의 경우 이미지 편집을 위해 전통적인 컴퓨터 비전(Computer Vision) 분야의 기법들을 거의 대부분 사용할 수 있습니다. 대표적으로는 이미지의 회전(Rotation), 크기 변환(Scaling), 밀림(Shearing), 상하좌우 대칭(Flip), 이동(Trnaslation), 잘라내기(Crop) 등이 있습니다. 이 중 하나의 과정만으로도 이미지의 **픽셀값 분포**가 꽤 극적으로 변하게 됩니다. 즉, 사람은 원본 이미지가 변형되었다는 것을 쉽게 알아챌 수 있지만, 컴퓨터의 경우 원본과 변형된 데이터가 원래 같은 것을 나타내는지 판단하기 매우 어렵다는 의미입니다. 이는 컴퓨터가 보기에 **서로 다른 두 데이터**가 새로 생긴 거나 마찬가지이므로 데이터 증강이 이루어졌다고 할 수 있습니다. 게다가 언급한 기법들을 여러개 섞어서 적용할 수도 있으므로 데이터를 증강할 수 있는 경우의 수는 그야말로 무궁무진하다고 할 수 있습니다.\n",
    "\n",
    "아래에서는 대부분의 딥러닝 모델에서 활용하는 기본적인 augmentation 방법인 회전(rotate), 좌우반전(flip), 잘라내기(crop) 등의 예를 살펴보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiq4HpIeW0V9"
   },
   "source": [
    "<center><img src=\"http://t1.kakaocdn.net/braincloud/homepage/article_image/98f65d67-431e-4d58-b269-5d9d287ea517.jpg\" alt=\"LeNet5\" align=\"center\" border=\"0\"  width=\"400\" height=auto></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **회전**: 말 그대로 원본 이미지를 일정 각도만큼 회전시키는 기법입니다. 같은 사물이라도 다양한 형태와 구도로 사진이 찍히기 때문에 회전을 통해 모델을 학습한다면 일반화 성능에 더 도움이 될 것입니다.\n",
    "\n",
    "<center><img src=\"./img/dataaug-rotation.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m4mpweaAZzpG"
   },
   "source": [
    "- **좌우반전**: 왼쪽의 고양이 원본 이미지에 대해 좌우반전을 한 모습입니다. 왼쪽의 이미지로만 분류하는 알고리즘을 학습시켰다면 오른쪽의 고양이 이미지를 입력 데이터로 넣었을 때 정확하게 분류하지 못하는 경우도 발생하죠. 사람의 눈에는 (어쩌면 똑같이 보일지도 모르는) 좌우반전만 한 이미지 차이지만 픽셀 단위로 모델이 데이터로 받아들이는 입장에서는 매우 큰 차이입니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tIsPxif4ZsuP"
   },
   "source": [
    "<center><img src=\"https://t1.daumcdn.net/cfile/tistory/995B3D395B62CED016\"\n",
    "alt=\"LeNet5\" align=\"center\" border=\"0\"  width=\"400\" height=auto></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1IP2laBakvr"
   },
   "source": [
    "-   **잘라내기**: 한 이미지에서 일부 영역만 잘라낸 부분을 패치(patch)라고 부르는데요. 아래와 같은 고양이 이미지에서 일부분의 영역을 잘라내서 데이터를 확보할 수도 있습니다. 적은 양의 데이터가 있을 때 많이 사용되는 방법이며 이미지의 부분만 보고도 분류할 수 있도록 풍부하게 데이터를 늘리는 방법입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UVn6tnNbnqa"
   },
   "source": [
    "<center><img src=\"https://t1.daumcdn.net/cfile/tistory/99E871435B62CED209\" alt=\"LeNet5\" align=\"center\" border=\"0\"  width=\"400\" height=auto></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KxBPTwH6bumY"
   },
   "source": [
    "이 외에도 이미지의 전체 크기는 고정시킨 후 상하좌우로 일정한 픽셀씩 밀기도 하는데 이러한 경우 비어있는 위치의 픽셀을 0으로 채우거나 밀려났던 픽셀의 값으로 채우기도 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 고급 이미지 데이터 증강 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**논문1**: [Improved Regularization of Convolutional Neural Networks with Cutout](https://arxiv.org/abs/1708.04552)\n",
    "\n",
    "**논문2**: [mixup: Beyond Empirical Risk Minimization (ICLR 2018)](https://arxiv.org/abs/1710.09412)\n",
    "\n",
    "**논문3**: [CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (ICCV 2019)](https://arxiv.org/abs/1905.04899)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이번에 알아볼 기법들은 단순 이미지 처리보다 좀더 복잡한 기법을 통해 이미지 증강을 수행하는 방법에 대해 알아보겠습니다. 말로는 복잡하다고 했지만, 어디까지나 단순 이미지 회전, 대칭 같은 과정보다 복잡할 뿐 아이디어 자체는 단순한 편입니다. 여기서 설명할 기법은 아래의 3가지가 있습니다.\n",
    "\n",
    "1. Cutout\n",
    "2. Mixup\n",
    "3. Cutmix\n",
    "\n",
    "이제 이 3가지 기법들을 하나씩 간단하게 알아보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Cutout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 **Cutout**은 주어진 각 이미지에서 임의의 영역을 아예 **제거**하는 기법입니다. 놀랍지만 이게 아이디어의 전부입니다. 일단 그림으로 나타내면 아래와 같습니다.\n",
    "\n",
    "<center><img src=\"./img/cutout.png\" width=400></center>\n",
    "\n",
    "그림을 보시면 아시겠지만, 각 그림의 일부 영역이 아예 회색 상자로 사라졌습니다. 이렇게 강제로 이미지 데이터의 일부를 손실시킨 것을 사용하여 이미지를 증강하는 것에 과연 어떤 의미가 있을까요?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutout의 의의를 좀 더 이해하기 위해서는 사전 연구들도 살펴볼 필요가 있습니다. 논문이 언급하는 사전연구로는 [Deep Learners Benefit More from Out-of-Distribution Examples](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.208.6380&rep=rep1&type=pdf), Dropout, [Denoising Auto-encoders](www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf)와 [Context Encoders](https://arxiv.org/abs/1604.07379)가 있습니다. 맨 처음 연구는 이미지에 임의의 낙서 같은 것을 추가하면 모델의 성능에 도움이 된다는 내용입니다. Dropout은 잘 아시리라 생각합니다. Cutout은 이 두 논문의 아이디어를 합쳐서 **임의의 영역에 해당하는 픽셀값을 0으로 바꾼 것**입니다. 즉, Cutout을 수행하면 일종의 정규화 효과가 더해져 성능이 향상될 것이라 가정했고 그게 그대로 들어맞은 것입니다.\n",
    "\n",
    "뒤의 두 논문(Encoder로 끝나는 논문) 또한 강제로 손상된 이미지로 Auto-Encoder라 불리는 모델을 학습하여 손상되기 이전의 이미지를 복원하는 내용의 논문입니다. 요약하면 기존에 입력 데이터에 임의의 손상이 발생한 경우 모델 성능이 좋아졌다는 사례들을 가지고 Cutout이라는 아이디어를 고안한 것이라 보시면 되겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. Mixup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음으로 알아볼 기법은 **Mixup**입니다. 이 기법은 하나의 학습 데이터를 구성하기 위해 데이터셋 내의 임의의 두 데이터를 적절한 비율로 **섞는(Mix)** 기법입니다. 수학적으로 표현하면 아래와 같습니다.\n",
    "\n",
    "$$\n",
    "\\tilde{x} = \\lambda x_{i} + (1 - \\lambda)x_{j} \\text{, where } x_{i} \\text{, } x_{j} \\text{ are raw input vectors}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{y} = \\lambda y_{i} + (1 - \\lambda)y_{j} \\text{, where } y_{i} \\text{, } y_{j} \\text{ are one-hot label encodings}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새로운 데이터 샘플을 생성하기 위해 $x_i$와 $x_j$를 $\\lambda : 1-\\lambda$의 비율로 섞고, 그에 따라 대응하는 label 데이터도 섞는 것을 확인할 수 있습니다. 이를 사진 예시로 표현하면 아래와 같습니다. \n",
    "\n",
    "<center><img src=\"./img/mixup.png\" width=600></center>\n",
    "\n",
    "그림에서는 $x_i$와 $x_j$가 각각 개와 고양이 그림이고 $\\lambda$는 0.7이라고 할 수 있습니다.\n",
    "\n",
    "그렇다면 이렇게 섞어서 생성한 데이터로 학습할 때 가지는 의미가 무엇일까요? 저자들은 기존의 딥러닝 모델 학습 과정에서는 **학습 데이터셋에 주어진 데이터의 분포만 보기 때문에** 모델 일반화에 한계가 있었다고 주장합니다. 이렇게 학습된 모델에 기존 학습 데이터셋의 분포와 **다른 분포의 데이터(Out-of-distribution)** 가 주어진다면 제대로 된 성능을 내기 어렵다는 것이 주장의 요지입니다. 이는 과적합(overfitting)과도 연결되는 주장이라고도 할 수 있겠죠. 따라서 mixup을 통해 새로 생성된 데이터는 훈련 데이터셋의 분포와 비슷하되 같지 않은 새로운 분포에서 온 데이터이기 때문에 모델의 일반화에 도움이 된다는 것입니다. 논문에서는 이를 근접(vicinal) 분포를 통한 리스크 최소화 방식이라 하여 **Vicinal Risk Minimization(VRM)** 이라는 용어를 통해 설명합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. Cutmix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 알아볼 기법은 **Cutmix**입니다. 이름에서 보면 벌써 감을 잡은 분도 계실 것 같습니다. 예상하신 대로 Cutout과 Mixup을 혼합한 기법입니다. 이 방식은 Cutout이 아예 데이터를 없애버리는 것과 Mixup으로 섞인 데이터가 부자연스럽다는 것을 단점으로 듭니다. 따라서 Cutmix는 Cutout을 통해 선택된 A 이미지의 일부 영역을 B 이미지의 또 다른 영역에 덮어씌우는 방식을 취합니다. 즉 A 이미지의 일부를 **복사**해서 B 이미지에 **붙여넣기** 한다고 보시면 됩니다.\n",
    "\n",
    "<center><img src=\"https://miro.medium.com/max/1400/1*AYyS08SHERhl_ZDB_wWtvg.jpeg\" width=200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "물론 이렇게 하여도 라벨은 Mixup과 같은 방식으로 합성됩니다. 이렇게 했을 때의 장점으로 저자들은 Cutout으로 사라진 영역에도 정보를 담고 있는 픽셀을 추가할 수 있게 되므로 정보 손실이 최소화된다는 것을 꼽습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Tensorflow로 알아보는 데이터 증강"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fWD7LZgvOMzj"
   },
   "source": [
    "이미지 데이터 증강을 위한 방법에는 1장 1단원에서 배운 함수들을 사용할 수도 있습니다. (PIL 라이브러리 함수들)\n",
    "\n",
    "아래에서는 Tensorflow에서 기본으로 지원하는 데이터셋과 이미지 변환 함수를 통해 데이터 증강 과정을 코드로 알아보도록 하겠습니다.\n",
    "\n",
    "Tensorflow에서 지원하는 데이터셋을 사용하기 위해서는 `tensorflow_datasets` 라이브러리를 설치해야 합니다. 플랫폼에 이미 설치가 되어 있으므로, 설치 과정은 생략하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrY3w07mqFFi"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHuUy7co3VO8"
   },
   "source": [
    "tensoflow에서 기본적으로 제공하는 데이터셋인 tfds 데이터를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202,
     "referenced_widgets": [
      "03981a06237246ab811869f1d6ec4bc9",
      "087a4f9757f74878839acb46d2ec10e0",
      "aca61d8b7f104639840525b12816634b",
      "7d3a2deb70404c6e84462290093cecbc",
      "b0284b41e52f4966a6c822feaad3744b",
      "337ef2b31bed4d5283ea8bb76a33bb36",
      "85ab06109b58466c9e7302d97360b9db",
      "ec433d06fb704d5dbc88cd4e3f82b028"
     ]
    },
    "id": "5jBKAch4O8sA",
    "outputId": "f1fdfdb5-91de-4559-a6c2-ac289a2b67cd"
   },
   "outputs": [],
   "source": [
    "(train_ds, val_ds, test_ds), metadata = tfds.load(\n",
    "    'tf_flowers',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s759Tw6U3wR4"
   },
   "source": [
    "불러온 데이터의 이미지와 라벨을 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "id": "lNECDaTYO_1-",
    "outputId": "71bcf011-81e2-4222-8290-8f5b66c99c68"
   },
   "outputs": [],
   "source": [
    "get_label_name = metadata.features['label'].int2str\n",
    "\n",
    "image, label = next(iter(train_ds))\n",
    "_ = plt.imshow(image)\n",
    "_ = plt.title(get_label_name(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY3_wSWN4PtU"
   },
   "source": [
    "`tf.keras.layers.experimental.preprocessing` 모듈에서는 레이어 단위에서 데이터전처리에 필요한 함수들을 제공합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "g0qB3k93PDZu",
    "outputId": "1a830774-9a4b-490e-aaf8-b6f241f71032"
   },
   "outputs": [],
   "source": [
    "# Rescaling을 통해 이미지 데이터의 각 픽셀 값의 범위를 [0, 255] 에서 [0.0, 1.0] 으로 변환합니다. \n",
    "# RandomFlip을 통해 수평 또는 수직으로 주어진 이미지를 flip합니다.\n",
    "# RandomRotation을 통해 이미지에 회전효과를 줍니다.\n",
    "\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.Rescaling(1/255),\n",
    "    layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "0qCJWVxpPE00",
    "outputId": "744d4c1c-2166-468d-e2fd-9d2e85749182"
   },
   "outputs": [],
   "source": [
    "# 이전 셀에서 증강시킨 이미지 데이터를 출력하여 어떤 변화가 있는지 확인해봅니다.\n",
    "image = tf.expand_dims(image, 0)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "    augmented_image = data_augmentation(image)\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(augmented_image[0])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpsKl5BFcnJf"
   },
   "source": [
    "## 2. 전이 학습 (Transfer Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"./img/google-scholar.png\" width=600></center>\n",
    "\n",
    "논문 검색을 위한 사이트 구글 스콜라에 접속하면 위와 같이 뉴턴의 **\"거인의 어깨에 올라서서 더 넓은 세상을 바라보라\"** 라는 문구가 등장합니다. 과학 분야에 인류사적으로 뛰어난 업적을 남긴 뉴턴도 자신의 성과가 이전에 축적된 수많은 연구들과 노력이 있었기에 가능했다고 고백하는 겸손함을 볼 수 있는 문구입니다. 이번에 다룰 전이 학습은 바로 이러한 모습과 유사하다고 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KihWsnBUw_n8"
   },
   "source": [
    "여담으로, 2016년에 AI 분야의 석학인 앤드류 응(Andrew Ng) 교수는 NIPS라는 저명한 학회에서 지도학습 이후로 전이학습이 머신러닝에서 대세가 될 것이라고 전망하기도 했습니다.\n",
    "<center><img src=\"./img/andrew-ng-nips-2016-transfer-learning.png\" width=400></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 전이 학습이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKPjrLtmfrKO"
   },
   "source": [
    "**전이 학습(Transfer Learning)** 은 학습에 필요한 데이터의 양이 충분하지 않아도 **사전에 훈련된 딥러닝 모델(pre-trained model)로 데이터의 일부만 재학습시켜** 원하는 목적에 필요한 모델을 만들어내는 기술입니다. 일반적으로 이미지 처리의 경우 사전 학습된 모델로 ImageNet으로 학습된 모델을 많이 사용합니다. ImageNet에 들어있는 이미지 개수가 100만개가 넘다 보니 특정 작업을 위해 준비한 데이터가 **비슷한 이미지 데이터**라면 사전 학습된 모델의 힘을 십분 활용할 수 있을 거라 기대하는 것입니다. 또한 가지고 있는 데이터 양이 적어도 비슷한 특징을 학습한 모델을 사용하니 효과적으로 모델을 재학습할 수 있을 것입니다.\n",
    "\n",
    "전이 학습에는 **source domain**과 **target domain**이란 개념이 등장합니다. 짐작하시겠지만, 사전 학습된 모델은 source domain의 데이터로 학습이 된 것이고 전이 학습은 해당 모델을 target domain에 재학습하는 과정이라 볼 수 있습니다. 따라서 전이 학습을 **Domain Adaptation** 과정이라 볼 수도 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7KymJWzfaKr"
   },
   "source": [
    "<center><img src=\"https://t1.daumcdn.net/cfile/tistory/993EB3335A01540D03\" alt=\"LeNet5\" align=\"center\" border=\"0\"  width=\"500\" height=auto></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "딥러닝 모델에서 전이 학습이 가능한 이유는 인공 신경망이 계층 구조로 이루어진 데에 있습니다. CNN의 경우 각 layer 별로 데이터로부터 추출하는 정보가 다르기도 하고 학습 가능 여부를 따로 설정할 수 있습니다. 이러한 점은 사전 학습된 모델에서 **우리가 원하는 부분만 사용하거나 재학습하는 것이 가능**하다는 의미가 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SaXqZ4SixpN8"
   },
   "source": [
    "전이 학습은 일반적으로 아래의 세가지 전략 중 하나를 택하여 수행하게 됩니다. 각 전략은 다음과 같습니다.\n",
    "1. **전체 모델 재훈련**: 사전 학습된 모델을 불러와 우리가 가진 데이터로 전체 모델을 재학습합니다.\n",
    "2. **일부 layer를 선택하여 학습**: 선택하지 않은 layer에서는 학습이 이루어지지 않습니다.\n",
    "3. **마지막 classifier만 학습**: Convolutional layer로 이루어진 모든 layer에서 학습이 이루어지지 않고 마지막 classifier만 새로 만들어 학습합니다.\n",
    "\n",
    "<center><img src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FszPak%2FbtqDE0n5MIL%2FpSKz0aeZZv2fkfo6Q2lhw0%2Fimg.png\"\n",
    "alt=\"LeNet5\" align=\"center\" border=\"0\"  width=\"700\" height=auto></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "셋 모두 layer를 재학습하는 것이라는 공통점이 있지만 2번째와 3번째 전략을 따로 **Fine Tuning (미세 조정)** 이라고 하여 부르는 경우가 많습니다. 특히 3번째 전략인 마지막 classifier, 즉 Fully-connected layer만 교체하여 Fine tuning 하는 경우가 굉장히 흔합니다. 이는 target domain에서 얻은 데이터셋의 class 개수와 사전 학습된 모델을 학습할 때 사용한 데이터셋의 class 개수가 다른 경우가 일반적이기 때문입니다.\n",
    "\n",
    "또한 반대로 학습이 이루어지지 않게 layer를 조정하는 작업을 layer를 **얼린다(Frozen)** 라고 표현합니다. 모델의 layer들이 가지고 있는 현재 값을 유지하게 만드는 것이므로 적절한 비유라고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. 전이 학습 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z7VqK_b_aTMt"
   },
   "source": [
    "이제 Tensorflow에서 제공하는 \"[고양이와 개 데이터셋](https://www.tensorflow.org/datasets/catalog/cats_vs_dogs)\"을 통해 전이 학습이 코드로 어떻게 이루어지는지 알아보도록 하겠습니다. 이 실습에서는 전이 학습의 3가지 전략 중 세번째 전략인 **마지막 classifier만 재학습**하는 전략을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0nDoG-29ITzj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "keras = tf.keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. 데이터셋 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dx65tkVraOxq",
    "outputId": "c81a4647-16a3-4e93-f015-d6b02fb601d7"
   },
   "outputs": [],
   "source": [
    "#training, validtation, test를 8:1:1로 분할합니다.\n",
    "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "1I4YLolVaO0s",
    "outputId": "98c10c00-f468-42c4-989d-0798fd3e03a2"
   },
   "outputs": [],
   "source": [
    "# training data에서 첫 두 개의 이미지를 샘플로 출력해봅니다.\n",
    "get_label_name = metadata.features['label'].int2str\n",
    "\n",
    "for image, label in raw_train.take(2):\n",
    "    plt.figure()\n",
    "    plt.imshow(image)\n",
    "    plt.title(get_label_name(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0L7iampaO3j"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 160 # 모든 이미지는 160x160으로 크기가 조정됩니다\n",
    "#format_example 함수를 통해 이미지 데이터를 같은 포맷으로 일괄적으로 통일시켜 줍니다.\n",
    "def format_example(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/127.5) - 1\n",
    "    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ayi8Nan1aO5y"
   },
   "outputs": [],
   "source": [
    "train = raw_train.map(format_example)\n",
    "validation = raw_validation.map(format_example)\n",
    "test = raw_test.map(format_example)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER_SIZE = 1000\n",
    "\n",
    "train_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "validation_batches = validation.batch(BATCH_SIZE)\n",
    "test_batches = test.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cFgl0r2gcSgi",
    "outputId": "f4ba01c6-d76b-4f3d-9d78-49040973f5c9"
   },
   "outputs": [],
   "source": [
    "for image_batch, label_batch in train_batches.take(1):\n",
    "    pass\n",
    "\n",
    "image_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. 사전 학습된 모델 불러오고 layer 얼리기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EEo_nCMubYXu"
   },
   "source": [
    "전이 학습을 위해 Google에서 개발한 MobileNetV2를 ImageNet으로 사전 훈련 시킨 모델을 사용할 것입니다. 실습에서는 앞서 언급한대로 전이 학습의 세번째 전략을 취하므로 **마지막 classifier만 fine-tuning 하기 위해** 사전 훈련된 모델에서 classifie 부분을 빼고 모델을 불러오도록 하겠습니다. 이는 `include_top`이란 argument를 False로 설정하면 되고 MobileNetV2 뿐만 아니라 tensorflow에서 API를 통해 제공하는 대부분의 모델에서 사용 가능한 옵션입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KTEdbyj9aO8g",
    "outputId": "346835d0-2d38-4220-b9a5-f64dd57b85fe"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# 사전 훈련된 모델 MobileNet V2에서 기본 모델을 생성합니다.\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fd_HI3WAaO-8",
    "outputId": "895241d9-25fc-42f7-bd13-0c693fb77a09"
   },
   "outputs": [],
   "source": [
    "# 이미지 배치가 사전 학습된 모델을 통과했을 때 출력하는 feature map의 크기를 출력합니다.\n",
    "feature_map_batch = base_model(image_batch)\n",
    "print(feature_map_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 불러온 모델의 모든 layer를 학습 가능하지 않게 설정해야 합니다. 즉, 현재 불러온 모델의 모든 layer를 **얼릴 필요**가 있습니다. 이를 위해 tensorflow는 모델과 layer에서 모두 사용 가능한 `trainable`이라는 속성(attribute)을 두고 있습니다. 말 그대로 학습 가능한지 여부를 True, False로 지정하면 되는 변수로 여기서는 학습이 되면 안되므로 False로 설정하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvMFjiA_aPBd"
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Vk7Rz99gaPDr",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7ef7bee2-9f84-406f-80a0-8671f4141aec",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 기본 모델 아키텍처를 살펴봅니다.\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. 새로 학습할 layer 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-p6nee0Odiz-"
   },
   "source": [
    "이제 불러온 개와 고양이 데이터셋에 fine tuning하기 위하여 새로 학습할 classifier를 추가할 차례입니다. 앞서 확인했듯 base 모델을 통과한 feature map이 여전히 2차원이기 때문에 이를 1차원으로 바꿔줘야 Fully-connected layer에 적용할 수 있을 것입니다. 아마 지금까지 강의를 따라오신 분들은 Flatten layer를 사용하는 것에 익숙할 것입니다. 하지만 여기서는 조금 다른 방식으로 1차원으로 바꿔보도록 하겠습니다.\n",
    "\n",
    "최근에 등장하고 있는 많은 CNN 모델은 Fully-connected layer로 feature map을 전달하기 위해 Flatten이 아닌 **Global Average Pooling** layer를 사용하는 추세입니다. 아시다시피 Flatten은 feature map 전체를 1차원으로 바꾸기 때문에 가지는 값의 개수가 굉장히 많아져 이어서 Fully-connected layer를 적용할 경우 parameter 수가 기하급수적으로 늘어나는 문제가 있습니다. Global Average Pooling layer는 각 차원별 feature map에서 전체 **2D 영역의 평균값을 계산하여 하나의 값**을 내게 됩니다. 결과적으로 **채널 차원만 남게 되어** 1차원으로 바뀌는 효과를 가지게 되고, 값의 개수 또한 극적으로 줄일 수 있습니다.\n",
    "\n",
    "<center><img src=\"./img/global-average-pooling.png\" width=500></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SY3-BwICaPGd",
    "outputId": "4b6ed79e-5de5-44fe-d3c5-c272b0effb5b"
   },
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_map_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4MtBUoudyzi"
   },
   "source": [
    "마지막으로 최종 classifier를 Fully-connected layer (`Dense`)를 통해 추가하도록 하겠습니다. 최종 출력값을 클래스가 1일 확률값(즉, dog일 확률)으로 해석할 것이므로 값을 한개만 출력하도록 뉴런 개수는 1개로 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KjLWNZ3EIT1d",
    "outputId": "29f7cc6d-5b94-44c0-93e5-676b425173c0"
   },
   "outputs": [],
   "source": [
    "prediction_layer = keras.layers.Dense(1)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D. 모델 Fine tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCqkBb5tfrTZ"
   },
   "source": [
    "`tf.keras.Sequential`을 사용하여 base 모델, global average pooling layer, classifier를 순서대로 쌓도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxUwP8WmIT4N"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "  prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoxX1MsLcwEH"
   },
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 fine tuning 이전의 성능을 확인하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNeZX7kccwOb",
    "outputId": "666a7a85-c515-45ad-b07b-8aab7c5ea051"
   },
   "outputs": [],
   "source": [
    "# Fine tuning 이전의 성능을 확인합니다.\n",
    "initial_epochs = 4\n",
    "validation_steps = 20\n",
    "\n",
    "loss_init, accuracy_init = model.evaluate(validation_batches, steps=validation_steps)\n",
    "print(\"initial loss: {:.2f}\".format(loss_init))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy_init))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 새로 추가한 모델을 fine tuning 하여 학습하도록 하겠습니다.\n",
    "\n",
    "학습 과정에서 `Corrupt JPEG data` 로 시작하는 경고와 `Warning: unknown JFIF revision number 0.00` 경고가 나타날 수 있습니다. <br>\n",
    "이 경고는 `Tensorflow` 에서 배포한 원본 데이터 중 손상된 데이터가 있다는 의미입니다. 학습 과정에서 손상된 데이터를 무시하기 때문에 모델 학습에는 영향을 끼치지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kr-ccFajc8SM",
    "outputId": "59419c1c-c85a-40b7-c3e5-02f7d11e27ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "history = model.fit(train_batches,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MExF6JIYc8Wx"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylabel('Cross Entropy')\n",
    "plt.ylim([0,1.0])\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MtxqGiwJB6BL"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hod8y3pmB35E"
   },
   "source": [
    "<span style=\"color:rgb(120, 120, 120)\">본 학습 자료를 포함한 사이트 내 모든 자료의 저작권은 엘리스에 있으며 외부로의 무단 복제, 배포 및 전송을 불허합니다.\n",
    "\n",
    "Copyright @ elice all rights reserved</span>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "[이론6] Data Augmentation and Transfer Learning - 1차 검수 반영",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "03981a06237246ab811869f1d6ec4bc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_aca61d8b7f104639840525b12816634b",
       "IPY_MODEL_7d3a2deb70404c6e84462290093cecbc"
      ],
      "layout": "IPY_MODEL_087a4f9757f74878839acb46d2ec10e0"
     }
    },
    "087a4f9757f74878839acb46d2ec10e0": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "337ef2b31bed4d5283ea8bb76a33bb36": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d3a2deb70404c6e84462290093cecbc": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ec433d06fb704d5dbc88cd4e3f82b028",
      "placeholder": "​",
      "style": "IPY_MODEL_85ab06109b58466c9e7302d97360b9db",
      "value": " 5/5 [00:03&lt;00:00,  1.36 file/s]"
     }
    },
    "85ab06109b58466c9e7302d97360b9db": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aca61d8b7f104639840525b12816634b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Dl Completed...: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_337ef2b31bed4d5283ea8bb76a33bb36",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b0284b41e52f4966a6c822feaad3744b",
      "value": 5
     }
    },
    "b0284b41e52f4966a6c822feaad3744b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "ec433d06fb704d5dbc88cd4e3f82b028": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
